{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ramin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/ramin/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ramin/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/ramin/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ramin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from Codes.Review_scrawler import *\n",
    "from Codes.feature_extraction import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('product_names.txt', \"r\") as x:\n",
    "#     all_product = x.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_name_description = Scrawl_product_description('product_names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('product_description.pickle', 'wb') as handle:\n",
    "#     pickle.dump(product_name_description, handle, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name_description = pickle.load(open('product_description.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 455/455 [00:00<00:00, 839968.45it/s]\n"
     ]
    }
   ],
   "source": [
    "sum_products = 0\n",
    "for key in tqdm(product_name_description.keys()):\n",
    "    sum_products += len(product_name_description[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The list of POS tags is as follows, with examples of what each POS stands for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CC coordinating conjunction\n",
    "* CD cardinal digit\n",
    "* DT determiner\n",
    "* EX existential there (like: “there is” … think of it like “there exists”)\n",
    "* FW foreign word\n",
    "* IN preposition/subordinating conjunction\n",
    "* JJ adjective ‘big’\n",
    "* JJR adjective, comparative ‘bigger’\n",
    "* JJS adjective, superlative ‘biggest’\n",
    "* LS list marker 1)\n",
    "* MD modal could, will\n",
    "* NN noun, singular ‘desk’\n",
    "* NNS noun plural ‘desks’\n",
    "* NNP proper noun, singular ‘Harrison’\n",
    "* NNPS proper noun, plural ‘Americans’\n",
    "* PDT predeterminer ‘all the kids’\n",
    "* POS possessive ending parent’s\n",
    "* PRP personal pronoun I, he, she\n",
    "* PRP possessive pronoun my, his, hers\n",
    "* RB adverb very, silently,\n",
    "* RBR adverb, comparative better\n",
    "* RBS adverb, superlative best\n",
    "* RP particle give up\n",
    "* TO, to go ‘to’ the store.\n",
    "* UH interjection, errrrrrrrm\n",
    "* VB verb, base form take\n",
    "* VBD verb, past tense took\n",
    "* VBG verb, gerund/present participle taking\n",
    "* VBN verb, past participle taken\n",
    "* VBP verb, sing. present, non-3d take\n",
    "* VBZ verb, 3rd person sing. present takes\n",
    "* WDT wh-determiner which\n",
    "* WP wh-pronoun who, what\n",
    "* WP possessive wh-pronoun whose\n",
    "* WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - Basic Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Step 1: Finding Candidate Features:__ First approach for extracting features was based on the intuition that product features are usually nouns or noun phrases [1]. Using the POS associated with words during the data generation phase, we created a new view of data where each sentence in the review was considered  as a bag of words. The  words  chosen  to  represent  a  sentence  were  those  that  were  marked  as  nouns  (NN/NNS).  We chose  to  ignore  proper  nouns,  which  we  believe  cannot  be  features  associated  with  a  product. \n",
    "\n",
    "* __Step 2: Finding Candidate Features:__ Furthermore, as we observed in the data, some phrases that represent features (such as optical zoom) were made of two classes of words, nouns and adjectives. So as to be able to detect such features, we also included words marked as adjectives (JJ/JJR/JJS) in our bag of words model.\n",
    "\n",
    "* __Step 3: Finding Frequent Features:__ The  next  step  was  to  extract  frequent  features  from  the  candidate  feature  words.  We  used  an implementation  based  on  the  APRIORI  algorithm  [10]  for  identifying  frequently  occurring word/word  pairs  from  the  bag  of  words  data  model. Using  a  support  threshold  of  0.5%,  we  got  a good  set  of  candidate  features.  \n",
    "\n",
    "* __Step 4: Remove Opinion Words:__ However,  we  observed  that  because  of  inclusion  of  adjectives  when finding the frequent item sets, we got many candidate features that actually were opinion words (like good,  best,  bad).  In  order  to  filter  out  such  frequent  items,  we  considered  only  those  single  items that  occurred  as a  noun  somewhere in  the  corpus, or  in  case  of  word phrases, if  at  least one  of the word in the phrase occurred as a noun somewhere in the corpus. \n",
    "\n",
    "* __Step 5: Feature Set:__ The resulting set of words was our feature set for the product trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Features(product_name_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 through 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ramin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/ramin/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ramin/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/ramin/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ramin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('premium', 'JJ'), ('full-grain', 'NN'), ('leather', 'NN'), ('upper', 'NN'), ('on', 'IN'), ('the', 'DT'), ('air', 'NN'), ('forc', 'VBZ'), ('1', 'CD'), ('low', 'JJ')]\n",
      "[('perfor', 'NN'), ('for', 'IN'), ('enhanc', 'NN'), ('ventil', 'NN')]\n",
      "[('pad', 'NN'), ('collar', 'NN'), ('for', 'IN'), ('a', 'DT'), ('snug', 'NN'), ('and', 'CC'), ('secur', 'NN')]\n",
      "[('foam', 'NN'), ('midsol', 'NN'), ('with', 'IN'), ('nike', 'JJ'), ('air', 'NN'), ('unit', 'NN'), ('for', 'IN'), ('lightweight', 'JJ'), ('cushion', 'NN')]\n",
      "[('pivot', 'NN'), ('point', 'NN'), ('in', 'IN'), ('the', 'DT'), ('forefoot', 'NN'), ('allow', 'NN'), ('for', 'IN'), ('smooth', 'JJ'), ('transit', 'NN'), ('in', 'IN'), ('all', 'DT'), ('direct', 'JJ')]\n",
      "[('non-mark', 'JJ'), ('rubber', 'NN'), ('outsol', 'NN'), ('for', 'IN'), ('durabl', 'NN'), ('traction', 'NN')]\n",
      "[('the', 'DT'), ('nike', 'JJ'), ('air', 'NN'), ('forc', 'VBD'), ('1', 'CD'), ('low', 'JJ'), ('is', 'VBZ'), ('import', 'NN')]\n",
      "[('nobodi', 'JJ'), ('ever', 'RB'), ('said', 'VBD'), ('that', 'IN'), ('be', 'VB'), ('an', 'DT'), ('icon', 'NN'), ('was', 'VBD'), ('easy', 'JJ'), ('.', '.'), ('that', 'WDT'), ('whi', 'VBD'), ('the', 'DT'), ('men', 'NNS'), ('nike', 'IN'), ('air', 'NN'), ('forc', 'VBP'), ('1', 'CD'), ('low', 'JJ'), ('casual', 'JJ'), ('shoe', 'NN'), ('work', 'NN'), ('hard', 'RB'), ('to', 'TO'), ('keep', 'VB'), ('you', 'PRP'), ('feel', 'VB'), ('and', 'CC'), ('look', 'VB'), ('your', 'PRP$'), ('best', 'JJS'), ('.', '.'), ('name', 'NN'), ('after', 'IN'), ('the', 'DT'), ('aircraft', 'NN'), ('that', 'WDT'), ('carri', 'VBZ'), ('the', 'DT'), ('u.s.', 'JJ'), ('presid', 'NN')]\n",
      "[('the', 'DT'), ('air', 'NN'), ('forc', 'VBZ'), ('1', 'CD'), ('continu', 'NN'), ('to', 'TO'), ('be', 'VB'), ('a', 'DT'), ('true', 'JJ'), ('leader', 'NN'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('of', 'IN'), ('footwear', 'NN'), ('.', '.')]\n",
      "[('lightweight', 'JJ'), ('knit', 'NN'), ('upper', 'JJ')]\n",
      "[('weld', 'NN'), ('overlay', 'NN'), ('are', 'VBP'), ('a', 'DT'), ('direct', 'JJ'), ('callout', 'NN'), ('to', 'TO'), ('the', 'DT'), ('origin', 'NN'), ('air', 'NN'), ('max', 'VBD'), ('97', 'CD'), ('and', 'CC'), ('provid', 'JJ'), ('durabl', 'NN')]\n",
      "[('3m', 'CD'), ('scotchlit', 'JJ'), ('detail', 'NN'), ('for', 'IN'), ('an', 'DT'), ('eye-catch', 'JJ'), ('look', 'NN')]\n",
      "[('full-length', 'JJ'), ('max', 'NNS'), ('air', 'NN'), ('unit', 'NN'), ('for', 'IN'), ('lightweight', 'NN')]\n",
      "[('respons', 'NNS'), ('cushion', 'NN')]\n",
      "[('rubber', 'NN'), ('outsol', 'NN'), ('for', 'IN'), ('durabl', 'NN'), ('traction', 'NN')]\n",
      "[('the', 'DT'), ('nike', 'JJ'), ('air', 'NN'), ('max', '$'), ('97', 'CD'), ('is', 'VBZ'), ('import', 'NN')]\n",
      "[('the', 'DT'), ('icon', 'NN'), ('air', 'NN'), ('max', 'VBD'), ('97', 'CD'), ('featur', 'JJ'), ('water-rippl', 'JJ'), ('line', 'NN')]\n",
      "[('reflect', 'NN'), ('pipe', 'NN'), ('and', 'CC'), ('full-length', 'JJ'), ('max', 'NN'), ('air', 'NN'), ('cushion', 'NN')]\n",
      "[('the', 'DT'), ('same', 'JJ'), ('design', 'NN'), ('that', 'WDT'), ('made', 'VBD'), ('the', 'DT'), ('men', 'NNS'), ('air', 'NN'), ('max', 'VBZ'), ('97', 'CD'), ('casual', 'JJ'), ('shoe', 'NN'), ('famous', 'JJ'), ('.', '.')]\n",
      "[('the', 'DT'), ('hoop', 'NN'), ('legend', 'VBP'), ('that', 'WDT'), ('chang', 'VBP'), ('the', 'DT'), ('game', 'NN'), ('forev', 'NN'), ('has', 'VBZ'), ('inspir', 'RB'), ('more', 'JJR'), ('than', 'IN'), ('a', 'DT'), ('few', 'JJ'), ('young', 'JJ'), ('baller', 'NN'), ('to', 'TO'), ('pick', 'VB'), ('up', 'RP'), ('the', 'DT'), ('game', 'NN'), ('.', '.'), ('he', 'PRP'), ('also', 'RB'), ('inspir', 'VBP'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('icon', 'JJ'), ('line', 'NN'), ('of', 'IN'), ('footwear', 'IN'), ('the', 'DT'), ('world', 'NN'), ('has', 'VBZ'), ('ever', 'RB'), ('seen', 'VBN'), ('.', '.'), ('continu', 'VB'), ('this', 'DT'), ('long', 'JJ'), ('legaci', 'NN')]\n",
      "[('the', 'DT'), ('men', 'NNS'), ('air', 'VBP'), ('jordan', 'NNS'), ('1', 'CD'), ('low', 'JJ'), ('basketbal', 'NN'), ('shoe', 'NN'), ('are', 'VBP'), ('made', 'VBN'), ('of', 'IN'), ('a', 'DT'), ('combin', 'NN'), ('leather', 'NN'), ('and', 'CC'), ('synthet', 'JJ'), ('upper', 'NN'), ('for', 'IN'), ('a', 'DT'), ('sporti', 'JJ'), ('look', 'NN'), ('that', 'WDT'), ('built', 'VBZ'), ('to', 'TO'), ('last', 'JJ'), ('.', '.'), ('the', 'DT'), ('air-sol', 'JJ'), ('unit', 'NN'), ('in', 'IN'), ('the', 'DT'), ('midsol', 'NN'), ('give', 'VB'), ('your', 'PRP$'), ('feet', 'NNS'), ('the', 'DT'), ('cushion', 'NN'), ('they', 'PRP'), ('demand', 'VBP')]\n",
      "[('while', 'IN'), ('the', 'DT'), ('flex', 'JJ'), ('groov', 'NN'), ('move', 'NN'), ('in', 'IN'), ('stride', 'NN'), ('with', 'IN'), ('you', 'PRP'), ('.', '.'), ('so', 'RB'), ('whether', 'IN'), ('you', 'PRP'), (\"'\", \"''\"), ('r', 'NN'), ('on', 'IN'), ('the', 'DT'), ('hardwood', 'NN')]\n",
      "[('blacktop', 'NN'), ('or', 'CC'), ('just', 'RB'), ('on', 'IN'), ('the', 'DT'), ('go', 'NN')]\n",
      "[('this', 'DT'), ('low-profil', 'JJ'), ('shoe', 'NN'), ('will', 'MD'), ('keep', 'VB'), ('you', 'PRP'), ('comfort', 'NN'), ('and', 'CC'), ('look', 'NN'), ('fresh', 'JJ'), ('.', '.')]\n",
      "[('features', 'NNS'), (':', ':')]\n",
      "[('upper', 'JJ'), (':', ':'), ('leather', 'NN'), ('and', 'CC'), ('mesh', 'NN')]\n",
      "[('outsole', 'NN'), (':', ':'), ('rubber', 'NN')]\n",
      "[('import', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "candidate_featres, pos = model.get_candidate_attributes(show = True, preprocess = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the data for Ramin and Jade to extract Rules from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ramin = np.random.choice(pos, 100)\n",
    "# jade = np.random.choice(pos, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('jade_pos.npy', jade)\n",
    "# np.save('ramin_pos.npy', ramin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create candidate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_counts = model.get_frequent_attributes(support = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nike air': 0.14071146245059288,\n",
       " 'air unit': 0.09189723320158102,\n",
       " 'rubber outsol': 0.1359683794466403,\n",
       " 'durabl traction': 0.06067193675889328,\n",
       " 'is import': 0.2490118577075099,\n",
       " 'casual shoe': 0.1409090909090909,\n",
       " 'air max': 0.10968379446640317,\n",
       " 'max air': 0.05849802371541502,\n",
       " 'run shoe': 0.061462450592885375,\n",
       " 'air vapormax': 0.05217391304347826}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of attributes and similar words using pretrained embedding matrix and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_similarity = model.get_similar_attributes(preprocess = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cushioning', 'outsole', 'midsole', 'insole', 'heel', 'color', 'shape', 'upper', 'fit', 'weight', 'density', 'fixation', 'collar', 'fasteners', 'permeability', 'stability', 'flexibility', 'traction', 'durability'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_similarity.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Attributes_dictionary_with_similarities.json', 'w') as handle:\n",
    "    json.dump(words_similarity, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ramin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/ramin/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ramin/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/ramin/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ramin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for x in ['hello', 'bad', 'good']:\n",
    "    z.append(word2vec.get_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(z, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
